{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys, email\n",
    "import pandas as pd \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('C:/Users/Hoc/OneDrive/My Documents/Machine Learning/kaggle/ames/')\n",
    "#os.chdir('C:/Users/Richard/OneDrive/My Documents/Machine Learning/kaggle/ames/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', na_values='NA')\n",
    "#print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Des</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>. . Residential Low Density zoning classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>. . Residential Low Density zoning classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>. . Residential Low Density zoning classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>. . Residential Low Density zoning classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>. . Residential Low Density zoning classificat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                                Des\n",
       "0   1  . . Residential Low Density zoning classificat...\n",
       "1   2  . . Residential Low Density zoning classificat...\n",
       "2   3  . . Residential Low Density zoning classificat...\n",
       "3   4  . . Residential Low Density zoning classificat...\n",
       "4   5  . . Residential Low Density zoning classificat..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################\n",
    "# Load Ames property cat descriptions dataset\n",
    "#########################################################\n",
    "\n",
    "\n",
    "# load enron dataset\n",
    "import pandas as pd \n",
    "df = pd.read_csv('des.csv',encoding=\"utf8\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "import string, re\n",
    "# clean up subject line\n",
    "df['Des'] = df['Des'].str.lower()\n",
    "df['Des'] = df['Des'].str.replace(r'[^a-z]', ' ')  \n",
    "df['Des'] = df['Des'].str.replace(r'\\s+', ' ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 284303\n",
      "Unique word count 328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 284303/284303 [00:00<00:00, 2182916.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['unknown', 0], ('of', 11337), ('the', 10930), ('typical', 9246), ('quality', 8473), ('average', 7858), ('on', 6750), ('location', 6734), ('exterior', 6699), ('basement', 6577)]\n",
      "Sample data [58, 84, 67, 59, 60, 1, 2, 11, 24, 112] ['residential', 'low', 'density', 'zoning', 'classification', 'of', 'the', 'sale', 'paved', 'alley']\n"
     ]
    }
   ],
   "source": [
    "# create sentence list \n",
    "df_text = df[\"Des\"].tolist()# + \". \" + df[\"Content\"])\n",
    "\n",
    "sentences = ' '.join(df_text)\n",
    "words = sentences.split()\n",
    "\n",
    "print('Data size', len(words))\n",
    " \n",
    "\n",
    "# get unique words and map to glove set\n",
    "print('Unique word count', len(set(words))) \n",
    " \n",
    "\n",
    "# drop rare words\n",
    "vocabulary_size = 500\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['unknown', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in tqdm(words):\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "\n",
    "del words  \n",
    "print('Most common words (+UNK)', count[:10])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [00:56, 39089.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 325 word vectors matching text data set.\n",
      "Total words in GloVe data set: 2196013\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# find matches with glove \n",
    "####################################################################\n",
    "GLOVE_DATASET_PATH = 'C:/Users/Hoc/Downloads/glove.840B.300d/glove.840B.300d.txt'\n",
    "\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DATASET_PATH, encoding=\"utf8\") #need to specify encoding or error.\n",
    "word_counter = 0\n",
    "for line in tqdm(f):\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  if word in dictionary:\n",
    "    #print(values[1:])\n",
    "    if '.com' in values[1]:\n",
    "        continue\n",
    "    else:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "  word_counter += 1\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors matching text data set.' % len(embeddings_index))\n",
    "print('Total words in GloVe data set: %s' % word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 155, 4: 57, 1: 56, 2: 30, 0: 27})\n",
      "\n",
      "\n",
      "\n",
      "Cluster: 3\n",
      "the and to of a in on with from was or not all has more one if just when some other new no than only over good two off home last down part between available site system s lot end family old water within car state hard quality side public story de members above below access face single short sale property hot major air half present living main interest clear inside section box material board terms purchase excellent feature court college yes estate quick unit regular trade gas sold allowed poor cash finish finished contract e described block warm score fair w completed payment properties mostly separate none etc rating forced sides roll university port officer attached minor mixed g privacy linked unknown split foundation covering warranty electricity rough tennis functionality damaged exceptional dot steam foreclosure amp floating quarters utilities allocation poured cracking deed deeds settling shakes tar zoning feeder deductions imitation miscellaneous breakers banked\n",
      "\n",
      "\n",
      "\n",
      "Cluster: 4\n",
      "room house building built floor homes wall pool kitchen metal garden wood stone interior walls roof garage residential heating constructed conditioning exterior vinyl fence shed tile basement brick fireplace condo clay villa wiring stove barn cement detached driveway furnace dwelling asbestos unfinished siding slab asphalt plywood veneer townhouse shingles foyer masonry stucco shingle prefabricated gable cinder mansard\n",
      "\n",
      "\n",
      "\n",
      "Cluster: 1\n",
      "high level type low common standard average levels normal significant positive heat condition associated shape slightly flat inches minimum rise medium grade height typically typical electrical exposure circuit wire tube severe configuration depression conventional hip slight gentle moderate density partial classification gravity composite severely assessed membrane heights abnormal knob irregular fuse moderately arterial wetness dampness postive\n",
      "\n",
      "\n",
      "\n",
      "Cluster: 2\n",
      "area near location town road land street park central south north corner west village east rail dirt adjacent railroad slope northwest creek alley gravel paved pavement adjoining meadow hillside frontage\n",
      "\n",
      "\n",
      "\n",
      "Cluster: 0\n",
      "nd ben rec sf sac iowa brook franklin edwards timberland mitchell cul gilbert somerset crawford bloomington sawyer greenbelt ames foyers northridge bluestem brookside mimimum romex unfinshed northpark\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "# Check out some clusters\n",
    "#########################################################\n",
    "\n",
    "# create a dataframe using the embedded vectors and attach the key word as row header\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(embeddings_index)\n",
    "dataframe = pd.DataFrame.transpose(dataframe)\n",
    " \n",
    "# See what it learns and look at clusters to pull out major themes in the data\n",
    "CLUSTER_SIZE = 5\n",
    "# cluster vector and investigate top groups\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=CLUSTER_SIZE)\n",
    "cluster_make = kmeans.fit_predict(dataframe)\n",
    "\n",
    "labels = kmeans.predict(dataframe)\n",
    "import collections\n",
    "cluster_frequency = collections.Counter(labels)\n",
    "print(cluster_frequency)\n",
    "cluster_frequency.most_common()\n",
    "\n",
    "clusters = {}\n",
    "n = 0\n",
    "for item in labels:\n",
    "    if item in clusters:\n",
    "        clusters[item].append(list(dataframe.index)[n])\n",
    "    else:\n",
    "        clusters[item] = [list(dataframe.index)[n]]\n",
    "    n += 1\n",
    "\n",
    "#Ideally we would vary cluster size, inpsect and assign meaningfull group names but do on fly instead:\n",
    "groups = []\n",
    "#print the top cluster_size most common clusters\n",
    "for k,v in cluster_frequency.most_common(100):\n",
    "  print('\\n\\n')\n",
    "  print('Cluster:', k)\n",
    "  print (' '.join(clusters[k]))\n",
    "  groups.append(clusters[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 1 3 3 3 3 3 2 3 3 3 3 3 3 3 4 3 4 3 3 1 3 1 3 3 3 2 3 3 3 1 3 3 3 3 4 3 3\n",
      " 3 3 3 3 3 3 1 3 3 3 2 2 3 2 1 3 3 3 1 2 4 1 3 3 3 4 3 3 3 4 3 3 1 3 3 3 1\n",
      " 3 4 3 3 1 1 3 1 1 3 2 4 4 4 3 2 3 3 3 3 3 3 4 3 3 3 2 3 1 3 4 3 2 1 2 2 1\n",
      " 3 1 3 1 1 3 1 3 3 2 3 1 3 3 2 3 3 1 1 4 1 3 2 3 4 3 4 3 3 1 1 1 3 1 1 3 4\n",
      " 1 3 3 4 4 1 3 1 4 3 3 1 3 3 3 3 1 3 2 4 2 1 4 1 3 1 1 4 4 2 4 3 1 4 3 4 3\n",
      " 4 4 4 4 4 3 3 4 1 4 1 4 1 0 1 1 4 1 2 1 3 3 4 2 4 2 4 2 1 3 3 3 3 4 4 2 2\n",
      " 4 1 3 1 1 2 0 2 4 4 3 3 3 1 4 2 4 3 4 3 3 0 0 4 1 2 4 2 0 0 4 4 4 3 0 2 4\n",
      " 0 4 0 1 4 3 0 0 0 4 0 1 4 0 0 1 0 0 0 0 0 0 4 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:27, 25015.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 419134 word vectors matching text data set.\n",
      "Total words in GloVe data set: 2196001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hoc\\AppData\\Local\\conda\\conda\\envs\\xgbDSv1\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working bag number: 0\n",
      "restorative\n",
      "phagemid\n",
      "cash\n",
      "wеrе\n",
      "juerga\n",
      "pasific\n",
      "reedobandito\n",
      "lasat\n",
      "equiprobable\n",
      "tabouli\n",
      "secluded\n",
      "gbaji\n",
      "frosts\n",
      "oxcart\n",
      "split\n",
      "chemi\n",
      "lounging\n",
      "disinfectors\n",
      "capcoms\n",
      "semiskilled\n",
      "zoning\n",
      "kingz\n",
      "husked\n",
      "bertuah\n",
      "amp\n",
      "telling\n",
      "bidentata\n",
      "millbank\n",
      "plethora\n",
      "cher\n",
      "functionality\n",
      "gamification\n",
      "hotrufftrade\n",
      "frontin\n",
      "violín\n",
      "cracking\n",
      "members\n",
      "has\n",
      "access\n",
      "sublevel\n",
      "pillowy\n",
      "testacea\n",
      "cms\n",
      "fays\n",
      "akashic\n",
      "jacinta\n",
      "inpulse\n",
      "independiente\n",
      "other\n",
      "conciencia\n",
      "gorse\n",
      "dionysusapplication\n",
      "fantagraphics\n",
      "lit\n",
      "nuyen\n",
      "conveying\n",
      "antiestrogen\n",
      "overlimit\n",
      "orgainzation\n",
      "rescue\n",
      "wvu\n",
      "donnee\n",
      "neuromasts\n",
      "diagrs\n",
      "thalaivar\n",
      "ryerson\n",
      "monteverde\n",
      "theron\n",
      "simultaneously\n",
      "liposafe\n",
      "openmsx\n",
      "aparecem\n",
      "trees\n",
      "éventuellement\n",
      "führen\n",
      "palen\n",
      "poxleitner\n",
      "several\n",
      "antidiabetes\n",
      "bellwood\n",
      "oversteering\n",
      "rofling\n",
      "breakers\n",
      "faroese\n",
      "fados\n",
      "lamarcus\n",
      "introtext\n",
      "greal\n",
      "spenny\n",
      "disguises\n",
      "hard\n",
      "nickles\n",
      "olimpo\n",
      "denatures\n",
      "evolves\n",
      "cremes\n",
      "sepration\n",
      "dieseases\n",
      "attainable\n",
      "aquaintences\n",
      "fennell\n",
      "absolue\n",
      "thhe\n",
      "thework\n",
      "benissa\n",
      "monophysite\n",
      "begrudges\n",
      "nitinol\n",
      "mimbar\n",
      "tits\n",
      "privacy\n",
      "nationalised\n",
      "clavicembalo\n",
      "on\n",
      "section\n",
      "public\n",
      "fayre\n",
      "starscasinos\n",
      "miscellaneous\n",
      "iré\n",
      "transmitting\n",
      "preselected\n",
      "lot\n",
      "turer\n",
      "karain\n",
      "triage\n",
      "e\n",
      "malodors\n",
      "playability\n",
      "poured\n",
      "creme\n",
      "makis\n",
      "acicularis\n",
      "oans\n",
      "responds\n",
      "duelo\n",
      "adherents\n",
      "thinpak\n",
      "conjunct\n",
      "regular\n",
      "micha\n",
      "delighted\n",
      "pitchability\n",
      "allowed\n",
      "circumventions\n",
      "picaresque\n",
      "widgetry\n",
      "anopheles\n",
      "attaches\n",
      "lifehacking\n",
      "newpct\n",
      "valleylist\n",
      "organizzare\n",
      "machinehole\n",
      "unskillfully\n",
      "referred\n",
      "jacking\n",
      "videodetective\n",
      "mosconi\n",
      "profita\n",
      "wern\n",
      "eham\n",
      "testimoni\n",
      "background\n",
      "constricting\n",
      "receivedst\n",
      "excellent\n",
      "clasical\n",
      "anxiety\n",
      "alwys\n",
      "solusinya\n",
      "pipius\n",
      "pmit\n",
      "moonchild\n",
      "uyumlu\n",
      "imbrace\n",
      "reheats\n",
      "salgo\n",
      "ohrringe\n",
      "w\n",
      "oktoberkind\n",
      "fatherland\n",
      "gvim\n",
      "rafters\n",
      "indicati\n",
      "truff\n",
      "off\n",
      "zlato\n",
      "enforce\n",
      "jameer\n",
      "offende\n",
      "cowherd\n",
      "cha\n",
      "cack\n",
      "naomily\n",
      "wees\n",
      "flexibile\n",
      "one\n",
      "cocunut\n",
      "or\n",
      "ehen\n",
      "half\n",
      "chioce\n",
      "pstart\n",
      "prints\n",
      "shiplap\n",
      "continous\n",
      "de\n",
      "pantai\n",
      "all\n",
      "extractum\n",
      "baseing\n",
      "schumer\n",
      "inanis\n",
      "enjoing\n",
      "surveilence\n",
      "jacobo\n",
      "verbalism\n",
      "hookin\n",
      "satoru\n",
      "borde\n",
      "barem\n",
      "sparkly\n",
      "cadence\n",
      "suggesting\n",
      "side\n",
      "outwitting\n",
      "picturehouse\n",
      "lactoglobulin\n",
      "classmates\n",
      "coppy\n",
      "writer\n",
      "successf\n",
      "performanc\n",
      "etc\n",
      "temperary\n",
      "spme\n",
      "interest\n",
      "inversely\n",
      "trailmaster\n",
      "financiers\n",
      "sendt\n",
      "motorvisioncom\n",
      "seaboards\n",
      "yachters\n",
      "jlb\n",
      "mostly\n",
      "xcursor\n",
      "finished\n",
      "subgrades\n",
      "sledge\n",
      "homebaked\n",
      "scalps\n",
      "seroflo\n",
      "when\n",
      "terrigenous\n",
      "applicatioyn\n",
      "ironworking\n",
      "ceoyoyo\n",
      "luker\n",
      "refuelling\n",
      "touge\n",
      "flystrike\n",
      "status\n",
      "utilities\n",
      "sedentariness\n",
      "with\n",
      "elimite\n",
      "postmillennialists\n",
      "leaves\n",
      "pardon\n",
      "payment\n",
      "exercises\n",
      "sacker\n",
      "deadpool\n",
      "chronoscaph\n",
      "if\n",
      "cajón\n",
      "feature\n",
      "recommitted\n",
      "fervidness\n",
      "knavery\n",
      "encyclo\n",
      "refritos\n",
      "chauffering\n",
      "meningococcal\n",
      "adjacent\n",
      "songtress\n",
      "hypocrisy\n",
      "widebody\n",
      "quarters\n",
      "constant\n",
      "speaks\n",
      "system\n",
      "eel\n",
      "geekbench\n",
      "justinspoliticalcorner\n",
      "thermidor\n",
      "msredneck\n",
      "floating\n",
      "reguli\n",
      "ansari\n",
      "patrons\n",
      "puddi\n",
      "kevingianni\n",
      "layettes\n",
      "minraise\n",
      "sheisty\n",
      "deductions\n",
      "adut\n",
      "reinstalling\n",
      "fanks\n",
      "gas\n",
      "property\n",
      "zizzle\n",
      "geun\n",
      "servicepersons\n",
      "seduces\n",
      "millo\n",
      "riuer\n",
      "backbreaker\n",
      "unison\n",
      "contará\n",
      "purifications\n",
      "stansted\n",
      "aprepitant\n",
      "entire\n",
      "wowww\n",
      "encantaaa\n",
      "tar\n",
      "tricyclo\n",
      "saratov\n",
      "zart\n",
      "clear\n",
      "places\n",
      "court\n",
      "lovebug\n",
      "anadi\n",
      "guessed\n",
      "machinesuperslots\n",
      "fantasys\n",
      "the\n",
      "josmiceli\n",
      "configuration\n",
      "gottex\n",
      "downloadengland\n",
      "sause\n",
      "bllack\n",
      "culmination\n",
      "magician\n",
      "privileged\n",
      "berbers\n",
      "innodb\n",
      "horribile\n",
      "breathplay\n",
      "sostiene\n",
      "theѕe\n",
      "dorming\n",
      "shopfitters\n",
      "reconciled\n",
      "yavapai\n",
      "balsawood\n",
      "unit\n",
      "dbooth\n",
      "comportemental\n",
      "something\n",
      "roscoe\n",
      "shapes\n",
      "besotes\n",
      "constamment\n",
      "separate\n",
      "disbarring\n",
      "tversity\n",
      "doylestown\n",
      "tacitly\n",
      "stina\n",
      "perrier\n",
      "nucleii\n",
      "casinosecashdirect\n",
      "asko\n",
      "pora\n",
      "suckandfuck\n",
      "overpayments\n",
      "narly\n",
      "lane\n",
      "coriaceous\n",
      "decentration\n",
      "telerobotics\n",
      "mrfrivolous\n",
      "sublattices\n",
      "breathtaking\n",
      "laminal\n",
      "gezicht\n",
      "thuat\n",
      "tennis\n",
      "externality\n",
      "etiolated\n",
      "amerihost\n",
      "invites\n",
      "deeds\n",
      "coltello\n",
      "keeping\n",
      "enterain\n",
      "single\n",
      "trade\n",
      "purchase\n",
      "settling\n",
      "nonacademic\n",
      "sisterlocks\n",
      "resistente\n",
      "hoteldeals\n",
      "promomix\n",
      "psychopharmacologist\n",
      "water\n",
      "madewell\n",
      "removed\n",
      "googling\n",
      "onಠ\n",
      "fastieslowie\n",
      "quality\n",
      "upgrate\n",
      "bullshittery\n",
      "pruritus\n",
      "pproximately\n",
      "disable\n",
      "rough\n",
      "elaborate\n",
      "dinho\n",
      "cholecystitis\n",
      "car\n",
      "mitzvoth\n",
      "brahmanas\n",
      "end\n",
      "kikay\n",
      "acceptance\n",
      "haud\n",
      "guayule\n",
      "zoya\n",
      "shadowgamer\n",
      "flubby\n",
      "northeners\n",
      "contract\n",
      "nighttrain\n",
      "air\n",
      "sare\n",
      "housebuyers\n",
      "slidding\n",
      "roof\n",
      "firehouses\n",
      "masterplan\n",
      "flet\n",
      "leafmould\n",
      "landholder\n",
      "confers\n",
      "deoxidized\n",
      "main\n",
      "board\n",
      "entrepreneurships\n",
      "tһe\n",
      "momentum\n",
      "milkstudios\n",
      "calibrownie\n",
      "exoticism\n",
      "factbox\n",
      "tatasky\n",
      "major\n",
      "halway\n",
      "dudet\n",
      "backups\n",
      "hossa\n",
      "jongreither\n",
      "rrv\n",
      "consolasatope\n",
      "enoughfor\n",
      "nonnegotiability\n",
      "ozzing\n",
      "attendant\n",
      "compasion\n",
      "noon\n",
      "shariff\n",
      "deshalb\n",
      "buenísimo\n",
      "living\n",
      "terms\n",
      "hurensohn\n",
      "tooken\n",
      "essent\n",
      "deptt\n",
      "fadings\n",
      "severability\n",
      "whyt\n",
      "university\n",
      "testrunner\n",
      "véritablement\n",
      "organisati\n",
      "kolby\n",
      "soomeone\n",
      "glassy\n",
      "unify\n",
      "fair\n",
      "golfing\n",
      "fwen\n",
      "foundation\n",
      "unprofitably\n",
      "poto\n",
      "fantasying\n",
      "material\n",
      "block\n",
      "oppression\n",
      "thiophosphate\n",
      "youz\n",
      "convicted\n",
      "wheatears\n",
      "shhhhhh\n",
      "nutin\n",
      "winpatrol\n",
      "screaming\n",
      "properties\n",
      "misera\n",
      "forumhome\n",
      "originals\n",
      "woukd\n",
      "not\n",
      "deed\n",
      "cleanroom\n",
      "unstimulated\n",
      "owning\n",
      "deregister\n",
      "homeownership\n",
      "fathering\n",
      "diterima\n",
      "countercyclical\n",
      "uppon\n",
      "validation\n",
      "inside\n",
      "maricón\n",
      "slde\n",
      "lingual\n",
      "neatly\n",
      "spiral\n",
      "rescource\n",
      "película\n",
      "rgs\n",
      "cancr\n",
      "celebrites\n",
      "baten\n",
      "calulated\n",
      "steam\n",
      "aemon\n",
      "ladino\n",
      "you\n",
      "yes\n",
      "pinoman\n",
      "sale\n",
      "teamtalk\n",
      "kavalan\n",
      "isochronism\n",
      "janusz\n",
      "pored\n",
      "xpedx\n",
      "desertion\n",
      "megara\n",
      "electricity\n",
      "matth\n",
      "applicatins\n",
      "pensions\n",
      "vulnerabilities\n",
      "ryley\n",
      "hot\n",
      "perforator\n",
      "profiting\n",
      "sower\n",
      "ravetube\n",
      "saxatilis\n",
      "augustifolia\n",
      "noscript\n",
      "enforced\n",
      "immigrants\n",
      "playvegasfromhomeapplication\n",
      "and\n",
      "fga\n",
      "doji\n",
      "speechcraft\n",
      "uke\n",
      "unknown\n",
      "doris\n",
      "pleistocene\n",
      "hammer\n",
      "monkery\n",
      "cuteboyswithcats\n",
      "estate\n",
      "demerits\n",
      "photomasks\n",
      "g\n",
      "xenpaks\n",
      "covering\n",
      "melanogaster\n",
      "gratings\n",
      "sustain\n",
      "manpack\n",
      "meddle\n",
      "home\n",
      "emloyees\n",
      "intensité\n",
      "clavamox\n",
      "sapidus\n",
      "ksort\n",
      "delimiters\n",
      "siner\n",
      "hesitations\n",
      "furnisher\n",
      "scruss\n",
      "bly\n",
      "mountd\n",
      "good\n",
      "linked\n",
      "incarnadine\n",
      "pilonidal\n",
      "dereliction\n",
      "college\n",
      "port\n",
      "eservice\n",
      "lukather\n",
      "lindstrom\n",
      "hellcats\n",
      "drogues\n",
      "intereresting\n",
      "cwnd\n",
      "looses\n",
      "biomicroscope\n",
      "down\n",
      "examination\n",
      "no\n",
      "civilazation\n",
      "pancrease\n",
      "electrophile\n",
      "magellanica\n",
      "mausers\n",
      "hornywomen\n",
      "tonges\n",
      "attached\n",
      "awfull\n",
      "sleding\n",
      "ulan\n",
      "deadener\n",
      "lamentable\n",
      "sprinkler\n",
      "box\n",
      "yaz\n",
      "more\n",
      "cioè\n",
      "unilever\n",
      "trust\n",
      "poor\n",
      "electrode\n",
      "carta\n",
      "short\n",
      "carde\n",
      "babes\n",
      "galunga\n",
      "coffer\n",
      "than\n",
      "valubale\n",
      "conceptuality\n",
      "icefields\n",
      "invitational\n",
      "substituting\n",
      "five\n",
      "resistances\n",
      "biobased\n",
      "watercolour\n",
      "necessities\n",
      "s\n",
      "talc\n",
      "a\n",
      "some\n",
      "frostproof\n",
      "dontbeafraidoftomorrow\n",
      "comptuers\n",
      "casinocreditcard\n",
      "tayu\n",
      "healable\n",
      "dot\n",
      "none\n",
      "respondin\n",
      "completed\n",
      "sides\n",
      "farrel\n",
      "wildhoney\n",
      "pneumatological\n",
      "pash\n",
      "obanno\n",
      "worryfree\n",
      "pilsner\n",
      "refrigeration\n",
      "fraction\n",
      "befit\n",
      "benna\n",
      "aussi\n",
      "criadores\n",
      "jazzes\n",
      "poole\n",
      "family\n",
      "uncurling\n",
      "octopus\n",
      "finish\n",
      "new\n",
      "place\n",
      "mattress\n",
      "acevement\n",
      "intercom\n",
      "northsea\n",
      "lasange\n",
      "celebrated\n",
      "concre\n",
      "frănsĭs\n",
      "two\n",
      "mixed\n",
      "adopters\n",
      "deviousness\n",
      "estube\n",
      "forced\n",
      "roll\n",
      "crona\n",
      "championsip\n",
      "volcanological\n",
      "jivey\n",
      "vulpecula\n",
      "rickards\n",
      "warranty\n",
      "pucks\n",
      "officer\n",
      "iinsurance\n",
      "hest\n",
      "bekker\n",
      "subclass\n",
      "kimchi\n",
      "eyesights\n",
      "ceremony\n",
      "diphenoxylate\n",
      "gonz\n",
      "villeins\n",
      "dip\n",
      "banked\n",
      "juast\n",
      "passability\n",
      "resampled\n",
      "to\n",
      "coldroom\n",
      "daclizumab\n",
      "nastere\n",
      "exceptional\n",
      "prophesizes\n",
      "bluestar\n",
      "simulater\n",
      "that\n",
      "declassifies\n",
      "them\n",
      "teruel\n",
      "staypozitive\n",
      "dirvers\n",
      "hairplugs\n",
      "inflows\n",
      "footbal\n",
      "tryer\n",
      "chce\n",
      "dhn\n",
      "aerotropolis\n",
      "greens\n",
      "materiel\n",
      "foreclosure\n",
      "imitation\n",
      "lyricsby\n",
      "face\n",
      "confettiandcocktails\n",
      "marae\n",
      "below\n",
      "itext\n",
      "longwearing\n",
      "perdues\n",
      "schönen\n",
      "dezincification\n",
      "present\n",
      "destiel\n",
      "available\n",
      "flourocarbon\n",
      "pavment\n",
      "damaged\n",
      "overusing\n",
      "rearranged\n",
      "pigmen\n",
      "pecks\n",
      "propagandists\n",
      "score\n",
      "was\n",
      "gustiness\n",
      "ysearch\n",
      "between\n",
      "from\n",
      "oganic\n",
      "laissez\n",
      "shll\n",
      "swithcing\n",
      "halfspaces\n",
      "jobseeker\n",
      "physiatry\n",
      "elected\n",
      "microcoil\n",
      "spoil\n",
      "angels\n",
      "durante\n",
      "deap\n",
      "just\n",
      "wksufreshair\n",
      "microirrigation\n",
      "deforciants\n",
      "allocation\n",
      "rating\n",
      "echosounders\n",
      "shakes\n",
      "bestimmten\n",
      "state\n",
      "downloaders\n",
      "riva\n",
      "lengthof\n",
      "youwould\n",
      "nonconformity\n",
      "synchroniser\n",
      "shuda\n",
      "teochew\n",
      "lobola\n",
      "only\n",
      "howtoplay\n",
      "ramage\n",
      "bonspiels\n",
      "esxupdate\n",
      "site\n",
      "part\n",
      "insight\n",
      "story\n",
      "occupyluxury\n",
      "in\n",
      "above\n",
      "old\n",
      "steersmanship\n",
      "kickplates\n",
      "guardin\n",
      "souffle\n",
      "loughborough\n",
      "unedifying\n",
      "alano\n",
      "overthinks\n",
      "tập\n",
      "flats\n",
      "unvarying\n",
      "feeder\n",
      "cpage\n",
      "bible\n",
      "krsna\n",
      "fermin\n",
      "strking\n",
      "birthmoms\n",
      "bridled\n",
      "quick\n",
      "over\n",
      "nonpro\n",
      "benchmark\n",
      "nécessaires\n",
      "hanok\n",
      "boonie\n",
      "cordered\n",
      "ramsac\n",
      "hispida\n",
      "escribe\n",
      "conoisseur\n",
      "holosight\n",
      "attorny\n",
      "enforcing\n",
      "recommmend\n",
      "within\n",
      "doree\n",
      "homebased\n",
      "ignoring\n",
      "curb\n",
      "described\n",
      "hemocoel\n",
      "resumes\n",
      "hideout\n",
      "sodering\n",
      "printed\n",
      "foth\n",
      "warm\n",
      "foxnews\n",
      "vibrater\n",
      "astate\n",
      "smartlink\n",
      "shadow\n",
      "wristlocks\n",
      "example\n",
      "sold\n",
      "of\n",
      "pid\n",
      "minor\n",
      "fightcard\n",
      "skycrane\n",
      "dispatched\n",
      "last\n",
      "stabed\n",
      "swines\n",
      "icodeforlove\n",
      "Working bag number: 1\n",
      "gloss\n",
      "vicforprez\n",
      "mzchine\n",
      "marinas\n",
      "fence\n",
      "phalanxes\n",
      "budded\n",
      "sourpuss\n",
      "cannot\n",
      "fetishistically\n",
      "skateboard\n",
      "quedarme\n",
      "reversible\n",
      "coordinates\n",
      "metal\n",
      "kodas\n",
      "goddesse\n",
      "bombshell\n",
      "sealable\n",
      "conneries\n",
      "dualities\n",
      "boleto\n",
      "repent\n",
      "wanks\n",
      "foscarini\n",
      "senko\n",
      "unprofitableness\n",
      "disk\n",
      "wall\n",
      "nutes\n",
      "atacked\n",
      "townhouse\n",
      "pledged\n",
      "preceeded\n",
      "opthamology\n",
      "foyer\n",
      "trustest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shingle\n",
      "selichot\n",
      "homes\n",
      "suggested\n",
      "tisa\n",
      "wiring\n",
      "veneer\n",
      "viridula\n",
      "asphalt\n",
      "committe\n",
      "farceur\n",
      "amontillado\n",
      "precipitously\n",
      "humax\n",
      "tradekey\n",
      "constructed\n",
      "highschoolers\n",
      "pool\n",
      "masonry\n",
      "fulmer\n",
      "omerta\n",
      "sybols\n",
      "building\n",
      "stdc\n",
      "villa\n",
      "pronouncable\n",
      "marchant\n",
      "dominating\n",
      "rescription\n",
      "solides\n",
      "vapour\n",
      "reticuloendothelial\n",
      "gige\n",
      "veils\n",
      "sized\n",
      "nomadism\n",
      "garage\n",
      "extending\n",
      "berkat\n",
      "ytou\n",
      "theunlovedbarbie\n",
      "dabbing\n",
      "unfinished\n",
      "gifpeanutbutter\n",
      "traff\n",
      "succesfuly\n",
      "decolletage\n",
      "oxxo\n",
      "driveway\n",
      "ardell\n",
      "withyou\n",
      "appers\n",
      "kaktus\n",
      "condo\n",
      "downstairs\n",
      "jacked\n",
      "furnace\n",
      "interior\n",
      "bionics\n",
      "muhammed\n",
      "oncology\n",
      "meeting\n",
      "exhaustiveness\n",
      "sule\n",
      "straplocks\n",
      "convenince\n",
      "seter\n",
      "stranger\n",
      "chronoscope\n",
      "floor\n",
      "impossilbe\n",
      "interwork\n",
      "thermoelement\n",
      "plently\n",
      "aigoo\n",
      "mansions\n",
      "reife\n",
      "rooms\n",
      "metalli\n",
      "shed\n",
      "billrubin\n",
      "azabicyclo\n",
      "toeach\n",
      "filers\n",
      "inspirationanaelle\n",
      "cement\n",
      "jodido\n",
      "subscriber\n",
      "nastier\n",
      "colourblock\n",
      "defition\n",
      "gable\n",
      "bedding\n",
      "birdcalls\n",
      "clay\n",
      "blue\n",
      "soudn\n",
      "stucco\n",
      "chevron\n",
      "studen\n",
      "gating\n",
      "stone\n",
      "fusili\n",
      "crazed\n",
      "cinder\n",
      "mansard\n",
      "exterior\n",
      "ended\n",
      "transformed\n",
      "spitted\n",
      "turboshaft\n",
      "männer\n",
      "longhaired\n",
      "snortdrugs\n",
      "ponying\n",
      "roxxi\n",
      "walls\n",
      "vinyl\n",
      "lalal\n",
      "toughest\n",
      "electroshock\n",
      "timebank\n",
      "indoor\n",
      "ingrid\n",
      "hjave\n",
      "chital\n",
      "affixes\n",
      "rockhall\n",
      "riblet\n",
      "built\n",
      "entryslotid\n",
      "informar\n",
      "conditioning\n",
      "coveres\n",
      "unstoppablekem\n",
      "quantile\n",
      "fatone\n",
      "shhould\n",
      "dita\n",
      "microtonal\n",
      "poophead\n",
      "tajmahal\n",
      "oder\n",
      "palmscasinos\n",
      "basement\n",
      "ponad\n",
      "effectuated\n",
      "itemprop\n",
      "ducting\n",
      "fireplace\n",
      "precooked\n",
      "knaller\n",
      "upstairs\n",
      "faculty\n",
      "thermophilic\n",
      "pixs\n",
      "nikken\n",
      "tejano\n",
      "plywood\n",
      "proph\n",
      "rogrammable\n",
      "reduct\n",
      "muchless\n",
      "toupe\n",
      "coral\n",
      "prefabricated\n",
      "contra\n",
      "getaways\n",
      "filey\n",
      "retrogoddess\n",
      "alpinestars\n",
      "onslaught\n",
      "najjaci\n",
      "crowdsource\n",
      "woozie\n",
      "superfruit\n",
      "honger\n",
      "glamorized\n",
      "youthfest\n",
      "soaringly\n",
      "baray\n",
      "fournisseurs\n",
      "kobiet\n",
      "authenticity\n",
      "candescent\n",
      "carbatrol\n",
      "fijian\n",
      "freewith\n",
      "vieille\n",
      "faver\n",
      "inviting\n",
      "machinesbonus\n",
      "casinoslotteri\n",
      "bertin\n",
      "flugelhorns\n",
      "chemoreception\n",
      "residential\n",
      "sneezes\n",
      "gotten\n",
      "casinoshot\n",
      "arsen\n",
      "prudential\n",
      "versuri\n",
      "hardjump\n",
      "colorfast\n",
      "millionthird\n",
      "garden\n",
      "sumwhere\n",
      "wood\n",
      "shingles\n",
      "glabella\n",
      "endrin\n",
      "warmth\n",
      "hunched\n",
      "slab\n",
      "dsx\n",
      "pastina\n",
      "carpark\n",
      "indexs\n",
      "reclamar\n",
      "elevators\n",
      "industires\n",
      "crashing\n",
      "asbestos\n",
      "lakeway\n",
      "chouinard\n",
      "lacetti\n",
      "haemodilution\n",
      "unfinalized\n",
      "plantations\n",
      "detached\n",
      "collecion\n",
      "holocaust\n",
      "memebrs\n",
      "deiodination\n",
      "disliked\n",
      "brick\n",
      "mandrin\n",
      "room\n",
      "gitarren\n",
      "gomoku\n",
      "viviparus\n",
      "heating\n",
      "hipper\n",
      "dwelling\n",
      "masques\n",
      "subscales\n",
      "roof\n",
      "stove\n",
      "killedbyart\n",
      "recrossed\n",
      "machinesoperation\n",
      "gelare\n",
      "tile\n",
      "gayageum\n",
      "airfield\n",
      "whipcream\n",
      "covern\n",
      "unshapely\n",
      "thermomix\n",
      "machinepokeri\n",
      "bodensee\n",
      "pombe\n",
      "berbere\n",
      "arhat\n",
      "imbalance\n",
      "adagios\n",
      "siger\n",
      "vivien\n",
      "kitchen\n",
      "companion\n",
      "leaker\n",
      "canibal\n",
      "relpy\n",
      "highjacked\n",
      "barn\n",
      "interest\n",
      "levs\n",
      "battaglie\n",
      "elm\n",
      "playback\n",
      "jcaho\n",
      "bondwomen\n",
      "obeedesigns\n",
      "proviruses\n",
      "hubbed\n",
      "dandelionsalad\n",
      "displaysto\n",
      "ensuite\n",
      "bars\n",
      "rightback\n",
      "œil\n",
      "cfares\n",
      "widdled\n",
      "naturalization\n",
      "house\n",
      "lombardia\n",
      "girling\n",
      "rasberry\n",
      "robby\n",
      "siding\n",
      "sobbing\n",
      "dobra\n",
      "Working bag number: 2\n",
      "consume\n",
      "supplis\n",
      "pstarr\n",
      "stalactitic\n",
      "wollastonite\n",
      "counselors\n",
      "triclinic\n",
      "dmso\n",
      "regulare\n",
      "söt\n",
      "scrivania\n",
      "meters\n",
      "subtropical\n",
      "sniggering\n",
      "sooty\n",
      "industrialist\n",
      "paintball\n",
      "clotting\n",
      "noun\n",
      "touro\n",
      "unfaith\n",
      "cmc\n",
      "density\n",
      "spyshot\n",
      "eager\n",
      "depression\n",
      "understand\n",
      "yayayoyo\n",
      "peripherin\n",
      "confides\n",
      "wetness\n",
      "ebed\n",
      "inocencia\n",
      "rainboy\n",
      "machineklondike\n",
      "slight\n",
      "precautious\n",
      "troilite\n",
      "rereward\n",
      "effiicient\n",
      "taler\n",
      "tieup\n",
      "beachten\n",
      "shuttle\n",
      "arterial\n",
      "wire\n",
      "mwd\n",
      "doohickies\n",
      "urien\n",
      "wests\n",
      "average\n",
      "toasty\n",
      "conventional\n",
      "crescendo\n",
      "darndest\n",
      "garnet\n",
      "wro\n",
      "rise\n",
      "legion\n",
      "renormalization\n",
      "slightly\n",
      "clearwell\n",
      "signatur\n",
      "lndry\n",
      "highest\n",
      "phrenological\n",
      "iyon\n",
      "pantes\n",
      "papermate\n",
      "sonar\n",
      "texa\n",
      "anathematizing\n",
      "consistent\n",
      "tranquilized\n",
      "leslie\n",
      "hiexam\n",
      "subcreation\n",
      "inversus\n",
      "letang\n",
      "dry\n",
      "doesnamounts\n",
      "znmeb\n",
      "citracal\n",
      "berkesan\n",
      "normal\n",
      "helpings\n",
      "insuficient\n",
      "associated\n",
      "postable\n",
      "zacky\n",
      "high\n",
      "telopeptide\n",
      "standard\n",
      "inability\n",
      "inches\n",
      "ayooo\n",
      "expresion\n",
      "positive\n",
      "level\n",
      "jocosely\n",
      "hindquarter\n",
      "severely\n",
      "irregular\n",
      "assessed\n",
      "buckypaper\n",
      "guts\n",
      "yoursef\n",
      "picachu\n",
      "loo\n",
      "dnj\n",
      "newzealand\n",
      "agayne\n",
      "brelan\n",
      "realizzazione\n",
      "common\n",
      "approximates\n",
      "reimbursement\n",
      "cheecks\n",
      "membrane\n",
      "tonsurans\n",
      "fuse\n",
      "hashanah\n",
      "lignan\n",
      "hydropathy\n",
      "supervolcano\n",
      "readers\n",
      "clikc\n",
      "syncprov\n",
      "motived\n",
      "behavorial\n",
      "hip\n",
      "undulation\n",
      "municipal\n",
      "ß\n",
      "graduand\n",
      "configuration\n",
      "gleevec\n",
      "momoxa\n",
      "prtg\n",
      "camoed\n",
      "gravity\n",
      "irishpogi\n",
      "burnishes\n",
      "reviewof\n",
      "vegasjoker\n",
      "jobing\n",
      "phlem\n",
      "typically\n",
      "parkii\n",
      "factitious\n",
      "sweden\n",
      "disbalance\n",
      "partial\n",
      "conwell\n",
      "focus\n",
      "wassailing\n",
      "verliefd\n",
      "sympathetic\n",
      "master\n",
      "dicken\n",
      "andesite\n",
      "primed\n",
      "sublimity\n",
      "woz\n",
      "jiah\n",
      "simillar\n",
      "smart\n",
      "sumped\n",
      "balvnced\n",
      "twinkle\n",
      "fabforgottennobility\n",
      "apparement\n",
      "confutation\n",
      "apoprotein\n",
      "cmmi\n",
      "gundog\n",
      "circles\n",
      "nose\n",
      "armas\n",
      "superstorm\n",
      "flatulency\n",
      "abnormal\n",
      "medium\n",
      "gives\n",
      "postive\n",
      "despite\n",
      "height\n",
      "anita\n",
      "predacious\n",
      "macclesfield\n",
      "aviao\n",
      "schenn\n",
      "neurophius\n",
      "eggstra\n",
      "babicz\n",
      "youcan\n",
      "wellbutin\n",
      "bister\n",
      "low\n",
      "databook\n",
      "deathmetal\n",
      "chichester\n",
      "tube\n",
      "circuit\n",
      "pucked\n",
      "ballet\n",
      "organising\n",
      "heights\n",
      "severe\n",
      "flat\n",
      "overeem\n",
      "moisturizer\n",
      "huged\n",
      "exmas\n",
      "longfiller\n",
      "littlerenocasino\n",
      "loomed\n",
      "niansomerhalder\n",
      "higher\n",
      "purposing\n",
      "dainty\n",
      "blackly\n",
      "muhsin\n",
      "mehdi\n",
      "perniciousness\n",
      "exposure\n",
      "callcenter\n",
      "angelcasino\n",
      "classification\n",
      "condition\n",
      "pharoh\n",
      "longipalpis\n",
      "utor\n",
      "consequat\n",
      "reviw\n",
      "headmaster\n",
      "ours\n",
      "type\n",
      "airnet\n",
      "triphop\n",
      "microprinting\n",
      "katoeys\n",
      "quality\n",
      "deelnemers\n",
      "datepart\n",
      "lexington\n",
      "aaahhhhhh\n",
      "thermocycler\n",
      "kronik\n",
      "tofurky\n",
      "levels\n",
      "nodi\n",
      "nessary\n",
      "planoconvex\n",
      "photod\n",
      "bmull\n",
      "projectile\n",
      "gentle\n",
      "occlusions\n",
      "sycophantically\n",
      "preschool\n",
      "shape\n",
      "winemaking\n",
      "violence\n",
      "statements\n",
      "apprisal\n",
      "workhorse\n",
      "gradating\n",
      "colectivos\n",
      "dammit\n",
      "significant\n",
      "boxercise\n",
      "sambol\n",
      "aequitas\n",
      "over\n",
      "gennaker\n",
      "phenmetrazine\n",
      "knob\n",
      "drony\n",
      "interesing\n",
      "electrical\n",
      "minimum\n",
      "typical\n",
      "gneiss\n",
      "lotion\n",
      "altyazı\n",
      "cholesteral\n",
      "collapsing\n",
      "desenho\n",
      "cfqueryparam\n",
      "scintillated\n",
      "viqgra\n",
      "floridana\n",
      "catalan\n",
      "remarries\n",
      "outshone\n",
      "chimichangas\n",
      "partycasinos\n",
      "ssd\n",
      "riprap\n",
      "grade\n",
      "evo\n",
      "rabit\n",
      "owing\n",
      "complaining\n",
      "engelbert\n",
      "recovers\n",
      "ahhaah\n",
      "antihistamines\n",
      "importantly\n",
      "composite\n",
      "paginator\n",
      "wracks\n",
      "apertures\n",
      "yourseld\n",
      "trashandtreasuree\n",
      "shoking\n",
      "dampness\n",
      "moderate\n",
      "aduki\n",
      "taeny\n",
      "commingle\n",
      "ocio\n",
      "subida\n",
      "moderately\n",
      "literraly\n",
      "malignancy\n",
      "swordmasters\n",
      "legendaries\n",
      "pundits\n",
      "ichor\n",
      "agame\n",
      "tollroads\n",
      "heat\n",
      "Working bag number: 3\n",
      "underparts\n",
      "injected\n",
      "area\n",
      "arredo\n",
      "nonvital\n",
      "homeyness\n",
      "portails\n",
      "thermapen\n",
      "adrenocorticotropin\n",
      "lurgy\n",
      "pavement\n",
      "straat\n",
      "doseages\n",
      "sjab\n",
      "oun\n",
      "northwest\n",
      "stigmas\n",
      "vuh\n",
      "view\n",
      "ofabeautifulnight\n",
      "athletes\n",
      "spreads\n",
      "perde\n",
      "multitype\n",
      "meaan\n",
      "octombrie\n",
      "nachie\n",
      "rail\n",
      "railroad\n",
      "thiel\n",
      "north\n",
      "adjacent\n",
      "evenals\n",
      "influencing\n",
      "near\n",
      "infants\n",
      "kitorang\n",
      "wonât\n",
      "hopup\n",
      "pwnguin\n",
      "imdw\n",
      "appraoched\n",
      "strechy\n",
      "blastema\n",
      "town\n",
      "rhodopsin\n",
      "candidiasis\n",
      "unhold\n",
      "stench\n",
      "park\n",
      "thinner\n",
      "areas\n",
      "ergos\n",
      "mainly\n",
      "inch\n",
      "swifty\n",
      "nonresident\n",
      "maturbate\n",
      "pjvideoguy\n",
      "tonuge\n",
      "predominantly\n",
      "south\n",
      "enmass\n",
      "latinateenporn\n",
      "quirkily\n",
      "deutschmark\n",
      "archiepiscopal\n",
      "leg\n",
      "scenerios\n",
      "nz\n",
      "meadow\n",
      "capos\n",
      "criticized\n",
      "street\n",
      "gleevec\n",
      "elites\n",
      "corner\n",
      "central\n",
      "casinospariswin\n",
      "east\n",
      "motifs\n",
      "culona\n",
      "location\n",
      "remnants\n",
      "sofrware\n",
      "courbes\n",
      "thespians\n",
      "west\n",
      "notations\n",
      "fnord\n",
      "inpatient\n",
      "surrounding\n",
      "hospitals\n",
      "undersells\n",
      "remainder\n",
      "applicationinstructions\n",
      "smartie\n",
      "wreathe\n",
      "localy\n",
      "creek\n",
      "alley\n",
      "land\n",
      "paved\n",
      "furloughed\n",
      "adjoining\n",
      "mirky\n",
      "casinotote\n",
      "bboying\n",
      "raaz\n",
      "contra\n",
      "flexable\n",
      "cctv\n",
      "blacki\n",
      "torana\n",
      "gravel\n",
      "precolored\n",
      "road\n",
      "vicinity\n",
      "windowsill\n",
      "hada\n",
      "daas\n",
      "nerdology\n",
      "pmviewed\n",
      "bcfd\n",
      "slope\n",
      "avors\n",
      "hillside\n",
      "octonions\n",
      "embosser\n",
      "but\n",
      "region\n",
      "desalinators\n",
      "sorest\n",
      "msvc\n",
      "stipulated\n",
      "égal\n",
      "wdiff\n",
      "libido\n",
      "boasting\n",
      "emilee\n",
      "felow\n",
      "illuminare\n",
      "employer\n",
      "narator\n",
      "triana\n",
      "peroxidase\n",
      "frontage\n",
      "tightline\n",
      "diffrerent\n",
      "balades\n",
      "sekaligus\n",
      "doogle\n",
      "boating\n",
      "guessing\n",
      "hablamos\n",
      "jakehollywood\n",
      "espression\n",
      "sundog\n",
      "servative\n",
      "doublet\n",
      "seedling\n",
      "deaver\n",
      "bibliothek\n",
      "innervision\n",
      "energyefficiency\n",
      "dirt\n",
      "contemperary\n",
      "pope\n",
      "riences\n",
      "telfer\n",
      "seaquake\n",
      "feed\n",
      "village\n",
      "tamarin\n",
      "nickelplated\n",
      "lettermans\n",
      "confucius\n",
      "bultaco\n",
      "exported\n",
      "Working bag number: 4\n",
      "ahhaa\n",
      "mitchell\n",
      "revels\n",
      "pattaya\n",
      "inmediata\n",
      "tablescasinos\n",
      "tht\n",
      "u\n",
      "rec\n",
      "potyvirus\n",
      "proteome\n",
      "edwards\n",
      "nirguna\n",
      "halfstep\n",
      "gasgas\n",
      "sns\n",
      "th\n",
      "biene\n",
      "rankfile\n",
      "hardeep\n",
      "claimants\n",
      "eicosanoids\n",
      "screming\n",
      "ispired\n",
      "bluestem\n",
      "gilbert\n",
      "looksy\n",
      "ruger\n",
      "belem\n",
      "studier\n",
      "taxed\n",
      "canisius\n",
      "somerset\n",
      "adultfun\n",
      "greenbelt\n",
      "flawlessness\n",
      "azygous\n",
      "delegacy\n",
      "otha\n",
      "northpark\n",
      "hylas\n",
      "devilment\n",
      "macaronic\n",
      "departmen\n",
      "sac\n",
      "tear\n",
      "affortable\n",
      "convertitore\n",
      "sawyer\n",
      "mantained\n",
      "miute\n",
      "subquestion\n",
      "oasis\n",
      "yanov\n",
      "civill\n",
      "leamy\n",
      "goodes\n",
      "undertood\n",
      "bbg\n",
      "checque\n",
      "njm\n",
      "ferid\n",
      "popularisation\n",
      "urmm\n",
      "withotu\n",
      "rahzel\n",
      "cul\n",
      "iowa\n",
      "pipefitter\n",
      "franklin\n",
      "abyssmal\n",
      "redear\n",
      "ferraris\n",
      "murg\n",
      "filarial\n",
      "advisor\n",
      "plaining\n",
      "attachable\n",
      "centiliter\n",
      "retread\n",
      "regretted\n",
      "paga\n",
      "moddii\n",
      "crawford\n",
      "bizare\n",
      "powercfg\n",
      "myeslf\n",
      "ben\n",
      "autotuner\n",
      "mimimum\n",
      "scrive\n",
      "cardbord\n",
      "preservable\n",
      "hearne\n",
      "electroreception\n",
      "strenght\n",
      "wenting\n",
      "companys\n",
      "jovi\n",
      "scarts\n",
      "indivdually\n",
      "brook\n",
      "waffies\n",
      "travertine\n",
      "ames\n",
      "quimby\n",
      "repertory\n",
      "foyers\n",
      "sequellae\n",
      "dimeric\n",
      "northridge\n",
      "neutralizer\n",
      "rocklandcasino\n",
      "sulfhydryls\n",
      "beaklike\n",
      "timberland\n",
      "mythologyofblue\n",
      "romex\n",
      "r\n",
      "growned\n",
      "evinced\n",
      "vraiblonde\n",
      "sigpro\n",
      "paziente\n",
      "goldengate\n",
      "phenprocoumon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teleworking\n",
      "mortgahe\n",
      "banged\n",
      "epergne\n",
      "mangroves\n",
      "eglington\n",
      "sf\n",
      "slotboss\n",
      "touraments\n",
      "molko\n",
      "wimmens\n",
      "reich\n",
      "bovver\n",
      "unfinshed\n",
      "bagi\n",
      "claussen\n",
      "createthislookforless\n",
      "gebeurd\n",
      "nd\n",
      "comtempt\n",
      "gimmik\n",
      "lumines\n",
      "bloomington\n",
      "draw\n",
      "concerete\n",
      "affirmations\n",
      "synchronizer\n",
      "ansi\n",
      "ethicist\n",
      "marcona\n",
      "theimaginationfoundation\n",
      "brookside\n",
      "polskie\n",
      "aptosid\n",
      "libya\n"
     ]
    }
   ],
   "source": [
    "# boost bags with cosine distance from full glove data set\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DATASET_PATH, encoding=\"utf8\")\n",
    "word_counter = 0\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "  # difference here as we don't intersect words, we take most of them\n",
    "    if '.com' in values[1]:\n",
    "        continue\n",
    "    elif (word.islower() and word.isalpha()): # work with smaller list of vectors\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    word_counter += 1\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors matching text data set.' % len(embeddings_index))\n",
    "print('Total words in GloVe data set: %s' % word_counter)\n",
    "\n",
    "# create a dataframe using the embedded vectors and attach the key word as row header\n",
    "glove_dataframe = pd.DataFrame(embeddings_index)\n",
    "glove_dataframe = pd.DataFrame.transpose(glove_dataframe)\n",
    "\n",
    "temp_matrix = pd.DataFrame.as_matrix(glove_dataframe)\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "\n",
    "vocab_boost_count = 5\n",
    "for group_id in range(len(groups)):\n",
    "  print('Working bag number:', str(group_id))\n",
    "  glove_dataframe_temp = glove_dataframe.copy()\n",
    "  vocab = []\n",
    "  for word in groups[group_id]:\n",
    "    print(word)\n",
    "    vocab.append(word)\n",
    "    cos_dist_rez = scipy.spatial.distance.cdist(temp_matrix, np.array(glove_dataframe.loc[word])[np.newaxis,:], metric='cosine')\n",
    "    # find closest words to help\n",
    "    glove_dataframe_temp['cdist'] = cos_dist_rez\n",
    "    glove_dataframe_temp = glove_dataframe_temp.sort_values(['cdist'], ascending=[1])\n",
    "    vocab = vocab + list(glove_dataframe_temp.head(vocab_boost_count).index)\n",
    "  # replace boosted set to old department group and remove duplicates\n",
    "  groups[group_id] = list(set(vocab))\n",
    "\n",
    "# save final objects to disk\n",
    "import pickle as pickle\n",
    "with open('full_bags.pk', 'wb') as handle:\n",
    "  pickle.dump(groups, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working bag number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:00, 3316.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working bag number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:00, 4876.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working bag number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:00, 4867.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working bag number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:00, 6840.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working bag number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [00:00, 6956.94it/s]\n",
      "1460it [00:00, 10438.23it/s]\n"
     ]
    }
   ],
   "source": [
    "##################################################################### \n",
    "# Create features of word counts for each item in each text\n",
    "#####################################################################\n",
    " \n",
    "import pickle as pickle\n",
    "with open('full_bags.pk', 'rb') as handle:\n",
    "    groups = pickle.load(handle)\n",
    "\n",
    "# loop through all texts and count group words in each raw text\n",
    "words_groups = []\n",
    "for group_id in range(len(groups)):\n",
    "  work_group = []\n",
    "  print('Working bag number:', str(group_id))\n",
    "  top_words = groups[group_id]\n",
    "  for index, row in tqdm(df.iterrows()): \n",
    "    text = (row[\"Des\"]) \n",
    "    work_group.append(len(set(top_words) & set(text.split())))\n",
    "    #work_group.append(len([w for w in text.split() if w in set(top_words)]))\n",
    "    \n",
    "  words_groups.append(work_group)\n",
    "\n",
    "# count emails per category group and feature engineering\n",
    "\n",
    "raw_text = []\n",
    "text_length = []\n",
    "text_word_count = []\n",
    "group_1 = []\n",
    "group_2 = []\n",
    "group_3 = []\n",
    "group_4 = []\n",
    "group_5 = []\n",
    "#group_6 = []\n",
    "#group_7 = []\n",
    "#group_8 = []\n",
    "#group_9 = []\n",
    "#group_10 = []\n",
    "#group_11 = []\n",
    "#group_12 = []\n",
    "#group_13 = []\n",
    "#group_14 = []\n",
    "#group_15 = []\n",
    "#group_16 = []\n",
    "#group_17 = []\n",
    "#group_18 = []\n",
    "#group_19 = []\n",
    "#group_20 = []\n",
    "#group_21 = []\n",
    "#group_22 = []\n",
    "#group_23 = []\n",
    "#group_24 = []\n",
    "#group_25 = []\n",
    "#group_26 = []\n",
    "#group_27 = []\n",
    "#group_28 = []\n",
    "#group_29 = []\n",
    "#group_30 = []\n",
    "\n",
    "final_outcome = []\n",
    "\n",
    "df['Des'].fillna('', inplace=True)\n",
    "\n",
    "counter = 0\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "  raw_text.append([row[\"Des\"]])\n",
    "  group_1.append(words_groups[0][counter])\n",
    "  group_2.append(words_groups[1][counter]) \n",
    "  group_3.append(words_groups[2][counter])\n",
    "  group_4.append(words_groups[3][counter])\n",
    "  group_5.append(words_groups[4][counter])\n",
    "  outcome_tots = [words_groups[0][counter], words_groups[1][counter], words_groups[2][counter],\n",
    "    words_groups[3][counter], words_groups[4][counter]] \n",
    "  final_outcome.append(outcome_tots.index(max(outcome_tots)))\n",
    "    \n",
    "  text_length.append(len(row['Des']))\n",
    "  text_word_count.append(len(row['Des'].split()))\n",
    " \n",
    "  counter += 1\n",
    "\n",
    "\n",
    "# add simple engineered features?\n",
    "training_set = pd.DataFrame({\n",
    "              \"raw_text\":raw_text,\n",
    "              \"group_1\":group_1,\n",
    "              \"group_2\":group_2,\n",
    "              \"group_3\":group_3,\n",
    "              \"group_4\":group_4,\n",
    "              \"group_5\":group_5,\n",
    "              \"text_length\":text_length,\n",
    "              \"text_word_count\":text_word_count,\n",
    "              \"outcome\":train.SalePrice})\n",
    "\n",
    "# might want to remove all texts that have all zeros (i.e. not from any of required categories)\n",
    "#training_set = training_set[\n",
    "#              (training_set.group_1 > 0) | \n",
    "#              (training_set.group_2 > 0) |\n",
    "#              (training_set.group_3 > 0) |\n",
    "#              (training_set.group_4 > 0) |\n",
    "#              (training_set.group_5 > 0)]\n",
    "#print(len(training_set))\n",
    "\n",
    "# save extractions to file\n",
    "training_set.to_csv('text_classification_df.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002DFBDB2E668>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\Hoc\\\\AppData\\\\Local\\\\Temp\\\\tmpdau9xwdr'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict_items' and 'dict_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-fbbab5510585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;31m# After reading in the data, you can train and evaluate the model:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\xgbDSv1\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                 instructions)\n\u001b[1;32m--> 488\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\xgbDSv1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[0;32m    523\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\xgbDSv1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks)\u001b[0m\n\u001b[0;32m   1037\u001b[0m       \u001b[0mrandom_seed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_random_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m       \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_or_create_global_step_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-fbbab5510585>\u001b[0m in \u001b[0;36mtrain_input_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meval_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-fbbab5510585>\u001b[0m in \u001b[0;36minput_fn\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     83\u001b[0m                       for k in CATEGORICAL_COLUMNS}\n\u001b[0;32m     84\u001b[0m   \u001b[1;31m# Merges the two dictionaries into one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m   \u001b[0mfeature_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontinuous_cols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m   \u001b[1;31m# Converts the label column into a constant Tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict_items' and 'dict_items'"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# TensorFlow Deep Classifier \n",
    "####################################################\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier\n",
    "# create a wide and deep model and also predict a few entries\n",
    "\n",
    "#Add Label column + extra columns to dataset prior to read in.\n",
    "model_ready_data = pd.read_csv('text_classification_df.csv') \n",
    "\n",
    "# https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
    "# (60% - train set, 20% - validation set, 20% - test set)\n",
    "df_train, df_test, df_val = np.split(model_ready_data.sample(frac=1), [int(.6*len(model_ready_data)), int(.8*len(model_ready_data))])\n",
    "\n",
    "# Continuous base columns\n",
    "text_length = tf.contrib.layers.real_valued_column(\"text_length\")\n",
    "text_word_count = tf.contrib.layers.real_valued_column(\"text_word_count\")\n",
    "\n",
    "group_1 = tf.contrib.layers.real_valued_column(\"group_1\")\n",
    "group_2 = tf.contrib.layers.real_valued_column(\"group_2\")\n",
    "group_3 = tf.contrib.layers.real_valued_column(\"group_3\")\n",
    "group_4 = tf.contrib.layers.real_valued_column(\"group_4\")\n",
    "group_5 = tf.contrib.layers.real_valued_column(\"group_5\")\n",
    "\n",
    "text_length_bucket = tf.contrib.layers.bucketized_column(text_length, boundaries=[900, 1200, 1500, 1800, 1900])\n",
    "\n",
    "# Categorical base columns\n",
    "#is_AM_sparse_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"is_AM\", keys=[\"yes\", \"no\"])\n",
    "# is_AM = tf.contrib.layers.one_hot_column(is_AM_sparse_column)\\\n",
    "#is_weekday_sparse_column = tf.contrib.layers.sparse_column_with_keys(column_name=\"is_weekday\", keys=[\"yes\", \"no\"])\n",
    "# is_weekday = tf.contrib.layers.one_hot_column(is_weekday_sparse_column)\n",
    "\n",
    "categorical_columns = [text_length_bucket] \n",
    "\n",
    "deep_columns = [text_length, text_word_count,\n",
    "               group_1, group_2, group_3, group_4, \n",
    "               group_5]\n",
    "\n",
    "simple_columns = [group_1, group_2, group_3, group_4, \n",
    "               group_5]\n",
    "\n",
    "import tempfile\n",
    "model_dir = tempfile.mkdtemp()\n",
    "regressor = tf.contrib.learn.DNNRegressor(feature_columns=simple_columns,\n",
    "                                hidden_units=[20, 20],\n",
    "                                #n_classes=6, # think only for classifiers\n",
    "                                optimizer=tf.train.ProximalAdagradOptimizer,\n",
    "                                #learning_rate=0.1,\n",
    "                                #l1_regularization_strength=0.001,\n",
    "                                model_dir=model_dir)\n",
    "\n",
    "# Define the column names for the data sets.\n",
    "COLUMNS = ['text_length',\n",
    " 'text_word_count',\n",
    " 'group_1',\n",
    " 'group_2',\n",
    " 'group_3',\n",
    " 'group_4',\n",
    " 'group_5',\n",
    " 'outcome']\n",
    "LABEL_COLUMN = 'outcome'\n",
    "#CATEGORICAL_COLUMNS = [\"is_AM\", \"is_weekday\"]\n",
    "CONTINUOUS_COLUMNS = ['text_length',\n",
    " 'text_word_count',\n",
    " 'group_1',\n",
    " 'group_2',\n",
    " 'group_3',\n",
    " 'group_4',\n",
    " 'group_5']\n",
    "\n",
    "#LABELS = [0, 1, 2, 3, 4, 5]\n",
    " \n",
    "def input_fn(df):\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values)\n",
    "                     for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {k: tf.SparseTensor(\n",
    "      indices=[[i, 0] for i in range(df[k].size)],\n",
    "      values=df[k].values,\n",
    "      dense_shape=[df[k].size, 1])\n",
    "                      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n",
    "  \n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label\n",
    "\n",
    "def train_input_fn():\n",
    "  return input_fn(df_train)\n",
    "\n",
    "def eval_input_fn():\n",
    "  return input_fn(df_test)\n",
    "# After reading in the data, you can train and evaluate the model:\n",
    "\n",
    "regressor.fit(input_fn=train_input_fn, steps=200)\n",
    "results = regressor.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "\n",
    "y_pred = regressor.predict(input_fn=lambda: input_fn(df_val), as_iterable=False)\n",
    "print(y_pred)\n",
    "\n",
    "print('buckets found:',set(y_pred))\n",
    "\n",
    "# # confusion matrix analysis\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(df_val[LABEL_COLUMN], y_pred)\n",
    "# pd.crosstab(df_val[LABEL_COLUMN], y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "# create some data\n",
    "# https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix\n",
    "\n",
    "#lookup = {0: 'group_1', 1:'group_2', 2:'group_3', 3:'group_4', 4:'group_5'}\n",
    "#y_truet = pd.Series([lookup[_] for _ in df_val[LABEL_COLUMN]])\n",
    "#y_predt = pd.Series([lookup[_] for _ in y_pred])\n",
    "#pd.crosstab(y_truet, y_predt, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['raw_text', 'group_1', 'group_2', 'group_3', 'group_4', 'group_5',\n",
       "       'text_length', 'text_word_count', 'outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ready_data = pd.read_csv('text_classification_df.csv')\n",
    "model_ready_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_ready_data.iloc[:,1:8] #NB. less final column! hence 1:8 not 1:7. (and 1 becuase we leave out raw text)\n",
    "Y = model_ready_data.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['group_1', 'group_2', 'group_3', 'group_4', 'group_5', 'text_length',\n",
       "       'text_word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.log1p(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12.247699\n",
       "1    12.109016\n",
       "2    12.317171\n",
       "3    11.849405\n",
       "4    12.429220\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "# increased epochs from 100 to 1000\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=1000, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.78 (0.15) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X.values, Y.values, cv=kfold, n_jobs=4)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
